Bugs (MUST DO ASAP)

1. # Bug only can scrap until page 115 when suppose to be at least 900+ pages - deep crawling (scraperr_multi_threading.py)


Todo list CURRENT LIST
- 99.co (Scrap the website)
    - main page (done)
    - subpage (pending)


Featuers
- Web Scrapping (https://rentinsingapore.com.sg/properties-for-rent) && (https://www.99.co)
    - main page (Done) 
    - Send to Json File (Done)
    - Scrap Mutiple Page (Done)
    - Full page 
    - Proxy Server
    - Multithreading (Done)
    - let user chose
    - maybe selanium
- Flask UI
- Marchine Learning 
- Read and Write JSON (Done)
- Data Cleaning (PENDING)
- Deep Crawling (Done)
- Data Comparison (PENDING)


- website data set is from current to 2020
Idea 
- scrap image also ?

# No-Async
# Scrapping :  139 seconds
# Deep Crawling : 1242 seconds

# Async 
# Scrapping : 14 seconds
# Deep Crawling : 711 seconds 

# Multi-threading
# Scrapping :
# Deep Crawling : 

# https://rentinsingapore.com.sg/rooms-for-rent
# https://rentinsingapore.com.sg/rooms-for-rent/page-1
# https://rentinsingapore.com.sg/ID679822


Seilium setup 
- had to install older version of chrome 
- https://chromedriver.chromium.org/downloads
- ChromeDriver 104.0.5112.79
- https://www.slimjet.com/chrome/google-chrome-old-version.php
- https://www.youtube.com/watch?v=ANVCyXcXs7Y&ab_channel=HalfG%C4%93k